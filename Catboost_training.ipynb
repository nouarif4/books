{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28b91ce9-4e22-475f-a354-e5c6d93f7157",
   "metadata": {},
   "source": [
    "we will decode the Categories and subcategories to use catboost in its full limits,because after a search we did we found out that CatBoost might misinterpret the binary-encoded values as having ordinal relationships and it will treat them as numerical features rather than categorical features. which means CatBoost won’t apply its specialized handling for categorical data (e.g., Ordered Target Encoding), which could reduce model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958c8519-8ca2-4877-bcfd-a4c983e9ff36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catboost in /usr/local/lib/python3.11/site-packages (1.2.7)\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/site-packages (from catboost) (0.20.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/site-packages (from catboost) (3.10.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.11/site-packages (from catboost) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/site-packages (from catboost) (2.2.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/site-packages (from catboost) (1.15.1)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.11/site-packages (from catboost) (6.0.0)\n",
      "Requirement already satisfied: six in /Users/haifaalsedairy/Library/Python/3.11/lib/python/site-packages (from catboost) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/haifaalsedairy/Library/Python/3.11/lib/python/site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas>=0.24->catboost) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/site-packages (from pandas>=0.24->catboost) (2025.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/site-packages (from matplotlib->catboost) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/site-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/site-packages (from matplotlib->catboost) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/site-packages (from matplotlib->catboost) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/haifaalsedairy/Library/Python/3.11/lib/python/site-packages (from matplotlib->catboost) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/site-packages (from matplotlib->catboost) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/site-packages (from matplotlib->catboost) (3.2.1)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /usr/local/lib/python3.11/site-packages (from plotly->catboost) (1.26.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/site-packages (3.1.5)\n",
      "Requirement already satisfied: xlrd in /usr/local/lib/python3.11/site-packages (2.0.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/haifaalsedairy/Library/Python/3.11/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/haifaalsedairy/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/haifaalsedairy/Library/Python/3.11/lib/python/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/site-packages (4.48.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/haifaalsedairy/Library/Python/3.11/lib/python/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2025.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/haifaalsedairy/Library/Python/3.11/lib/python/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (2025.1.31)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/site-packages (1.26.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/site-packages (from scikit-learn) (1.15.1)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.6.1-cp311-cp311-macosx_10_9_x86_64.whl (12.1 MB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 threadpoolctl-3.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install catboost\n",
    "%pip install pandas openpyxl xlrd\n",
    "%pip install torch\n",
    "%pip install transformers\n",
    "%pip install numpy\n",
    "%pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a45d72-10fd-43b9-b390-c8f48d0a3e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "# Load dataset\n",
    "file_path = \"/Users/haifaalsedairy/Desktop/books/Cleaned Dataset/Book_Cleaned_Dataset_.xls\"\n",
    "df = pd.read_csv(file_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze()\n",
    "        }\n",
    "\n",
    "def convert_to_embeddings(df, column_names, max_length=512, batch_size=32, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "  \n",
    "    # Load model and tokenizer once\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained('asafaya/bert-base-arabic')\n",
    "    model = AutoModel.from_pretrained('asafaya/bert-base-arabic')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    for column_name in column_names:\n",
    "        print(f\"\\nProcessing column: {column_name}\")\n",
    "        \n",
    "        # Create dataset and dataloader\n",
    "        texts = df[column_name].tolist()\n",
    "        dataset = TextDataset(texts, tokenizer, max_length)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # Process batches\n",
    "        embeddings = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Generating embeddings\"):\n",
    "                # Move batch to device\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                \n",
    "                # Generate embeddings\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                \n",
    "                # Compute mean pooling\n",
    "                mask = attention_mask.unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()\n",
    "                masked_embeddings = outputs.last_hidden_state * mask\n",
    "                summed = torch.sum(masked_embeddings, 1)\n",
    "                counts = torch.clamp(mask.sum(1), min=1e-9)\n",
    "                mean_pooled = summed / counts\n",
    "                \n",
    "                # Move to CPU and convert to numpy\n",
    "                embeddings.append(mean_pooled.cpu().numpy())\n",
    "        \n",
    "        # Concatenate all batches\n",
    "        all_embeddings = np.concatenate(embeddings, axis=0)\n",
    "        \n",
    "        # Store embeddings in DataFrame\n",
    "        df[f\"{column_name}_embedded\"] = list(all_embeddings)\n",
    "        \n",
    "        print(f\"Completed embedding generation for {column_name}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed616166-8a1b-44e2-89c1-9637539bd5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Category\n",
      "0     10000\n",
      "1        10\n",
      "2  10000000\n",
      "   Category\n",
      "0     10000\n",
      "1        10\n",
      "2  10000000\n",
      "  Category_original\n",
      "0  الصحافة والإعلام\n",
      "1   الكتب الإسلامية\n",
      "2     الأسرة والطفل\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def decode_categories(df):\n",
    "\n",
    "    # Define the category mapping\n",
    "    category_map = {\n",
    "        \"الأدب والخيال\": \"1\",\n",
    "        \"الكتب الإسلامية\": \"10\",\n",
    "        \"الاقتصاد والأعمال\": \"100\",\n",
    "        \"الفلسفة\": \"1000\",\n",
    "        \"الصحافة والإعلام\": \"10000\",\n",
    "        \"الكتب السياسية\": \"100000\",\n",
    "        \"العلوم والطبيعة\": \"1000000\",\n",
    "        \"الأسرة والطفل\": \"10000000\",\n",
    "        \"السير والمذكرات\": \"100000000\",\n",
    "        \"الفنون\": \"1000000000\",\n",
    "        \"التاريخ والجغرافيا\": \"10000000000\",\n",
    "        \"الرياضة والتسلية\": \"100000000000\",\n",
    "        \"الشرع والقانون\": \"1000000000000\"\n",
    "    }\n",
    "    \n",
    "    # Create reversed mapping\n",
    "    reversed_category_map = {v: k for k, v in category_map.items()}\n",
    "    \n",
    "    # Convert category values to string to ensure proper matching\n",
    "    df['Category'] = df['Category'].astype(str)\n",
    "    \n",
    "    # Function to safely map categories\n",
    "    def safe_map_category(x):\n",
    "        if pd.isna(x) or x == 'nan':\n",
    "            return np.nan\n",
    "        \n",
    "        # Convert the input to a simple string of the number\n",
    "        x_str = str(int(x))  # This removes leading zeros and converts to simple number string\n",
    "        \n",
    "        return reversed_category_map.get(x_str, x)\n",
    "    \n",
    "    # Apply the mapping\n",
    "    df['Category_original'] = df['Category'].apply(safe_map_category)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(df[['Category']].head(3))\n",
    "df = decode_categories(df)\n",
    "print(df[['Category']].head(3))\n",
    "print(df[['Category_original']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa686ccd-2f19-4eef-acc2-3033a43a31aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Subcategory Subcategory_original\n",
      "0           65   الندوات والمؤتمرات\n",
      "1           54        القرآن وعلومه\n",
      "2           71          شؤون المرأة\n",
      "3           74           علم النبات\n",
      "4           68          تاريخ الأدب\n"
     ]
    }
   ],
   "source": [
    "# Load the encoder later\n",
    "with open(\"label_encoder_Subcategory.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "    \n",
    "# Decode the 'Subcategory' column back to original values\n",
    "df['Subcategory_original'] = label_encoder.inverse_transform(df['Subcategory'])\n",
    "\n",
    "# Display a sample of the reversed data\n",
    "print(df[['Subcategory', 'Subcategory_original']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c65f978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    3299.000000\n",
      "mean       94.642316\n",
      "std        95.139226\n",
      "min         2.000000\n",
      "25%        56.000000\n",
      "50%        63.000000\n",
      "75%        72.000000\n",
      "max      1374.000000\n",
      "Name: word_count_Description, dtype: float64\n",
      "count    3299.000000\n",
      "mean        4.825402\n",
      "std         2.597022\n",
      "min         1.000000\n",
      "25%         3.000000\n",
      "50%         4.000000\n",
      "75%         6.000000\n",
      "max        20.000000\n",
      "Name: word_count_Title, dtype: float64\n",
      "Loading model and tokenizer...\n",
      "\n",
      "Processing column: Title\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 104/104 [01:18<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed embedding generation for Title\n",
      "Loading model and tokenizer...\n",
      "\n",
      "Processing column: Description\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 104/104 [13:14<00:00,  7.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed embedding generation for Description\n"
     ]
    }
   ],
   "source": [
    "# we will decide the max_length based on the following results \n",
    "# Calculate the number of words in each text\n",
    "df['word_count_Description'] = df['Description'].apply(lambda x: len(x.split()))\n",
    "df['word_count_Title'] = df['Title'].apply(lambda x: len(x.split()))\n",
    "# Analyze the distribution\n",
    "print(df['word_count_Description'].describe())\n",
    "print(df['word_count_Title'].describe())\n",
    "df.drop(['word_count_Title', 'word_count_Description'], axis=1)\n",
    "\n",
    "df = convert_to_embeddings(df, \n",
    "                         column_names=['Title'], \n",
    "                         max_length=20, \n",
    "                         batch_size=32)\n",
    "#%75 of descriptions will be covered and 128 will avoid excessive padding for shorter descriptions\n",
    "# and will truncates very long descriptions\n",
    "df = convert_to_embeddings(df, \n",
    "                         column_names=[\"Description\"], \n",
    "                         max_length=128, \n",
    "                         batch_size=32)\n",
    "\n",
    "#Flatten embeddings into separate columns\n",
    "df = pd.concat([df.drop(['Title', 'Description'], axis=1),\n",
    "                df['Title'].apply(pd.Series),\n",
    "                df['Description'].apply(pd.Series)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8438b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Author  Pages  Publication year  Publisher Category  Subcategory  Price  \\\n",
      "0    2073     40              2003        145    10000           65  16.88   \n",
      "\n",
      "  Page Range Category_original Subcategory_original  word_count_Description  \\\n",
      "0       0-50  الصحافة والإعلام   الندوات والمؤتمرات                      71   \n",
      "\n",
      "   word_count_Title                                     Title_embedded  \\\n",
      "0                 7  [0.64062047, -0.5985902, 0.09329649, -0.492798...   \n",
      "\n",
      "                                Description_embedded  \\\n",
      "0  [0.47870982, -0.15091096, 0.24697201, -0.48251...   \n",
      "\n",
      "                                                 0  \\\n",
      "0  التشبيك وميثاق الممارسة في عمل المنظمات الأهلية   \n",
      "\n",
      "                                                   0  \n",
      "0  تقرير يوثق أعمال ورشة عمل 1995 عن محاولة صياغة...  \n"
     ]
    }
   ],
   "source": [
    "# the end result of the dataset in the training we will use Subcategory_original, Category_original\n",
    "# in catogory format after decoding to utilize catboost \n",
    "print (df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc611ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 2.5328894\ttotal: 18.1s\tremaining: 5h 1m 17s\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Prepare the dataset for CatBoost\n",
    "# Use only the embeddings for Description as features and Category_original as the target\n",
    "X = df['Description_embedded'].apply(np.array).tolist()  # Only use Description embeddings as features\n",
    "X = np.array(X)  # Convert the list to a numpy array\n",
    "\n",
    "# The target variable is 'Category_original'\n",
    "y = df['Category_original']\n",
    "\n",
    "# 2. Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Initialize the CatBoost model\n",
    "model = CatBoostClassifier(iterations=1000,  # Number of trees\n",
    "                          depth=10,  # Depth of each tree\n",
    "                          learning_rate=0.05,  # Learning rate\n",
    "                          loss_function='MultiClass',  # Multi-class classification\n",
    "                          cat_features=[]  # No categorical features in this case\n",
    "                          )\n",
    "\n",
    "# 4. Train the model\n",
    "model.fit(X_train, y_train, verbose=200)  # Print training progress every 200 iterations\n",
    "\n",
    "# 5. Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 6. Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# 7. Classify a new description (example)\n",
    "example_description = \"قصص مغامرات للأطفال\"  # Example description\n",
    "example_embedding = tokenizer(example_description, truncation=True, max_length=128, padding='max_length', return_tensors='pt').to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(example_embedding['input_ids'], attention_mask=example_embedding['attention_mask'])\n",
    "    mask = example_embedding['attention_mask'].unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()\n",
    "    masked_embeddings = outputs.last_hidden_state * mask\n",
    "    summed = torch.sum(masked_embeddings, 1)\n",
    "    counts = torch.clamp(mask.sum(1), min=1e-9)\n",
    "    mean_pooled = summed / counts\n",
    "    example_embedding = mean_pooled.cpu().numpy()\n",
    "\n",
    "# Predict the category for the new description\n",
    "example_category = model.predict(example_embedding)\n",
    "print(f\"Predicted Category: {example_category}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
