{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28b91ce9-4e22-475f-a354-e5c6d93f7157",
   "metadata": {},
   "source": [
    "we will decode the Categories and subcategories to use catboost in its full limits,because after a search we did we found out that CatBoost might misinterpret the binary-encoded values as having ordinal relationships and it will treat them as numerical features rather than categorical features. which means CatBoost won’t apply its specialized handling for categorical data (e.g., Ordered Target Encoding), which could reduce model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "958c8519-8ca2-4877-bcfd-a4c983e9ff36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catboost in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.2.7)\n",
      "Requirement already satisfied: graphviz in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from catboost) (0.20.3)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from catboost) (3.10.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from catboost) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.24 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from catboost) (2.2.3)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from catboost) (1.15.1)\n",
      "Requirement already satisfied: plotly in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from catboost) (6.0.0)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas>=0.24->catboost) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas>=0.24->catboost) (2024.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->catboost) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->catboost) (4.55.8)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->catboost) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->catboost) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->catboost) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->catboost) (3.1.4)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from plotly->catboost) (1.26.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: openpyxl in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.1.5)\n",
      "Requirement already satisfied: xlrd in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.0.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: et-xmlfile in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.48.3)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2025.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (2022.12.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.26.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install catboost\n",
    "%pip install pandas openpyxl xlrd\n",
    "%pip install torch\n",
    "%pip install transformers\n",
    "%pip install numpy\n",
    "%pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a45d72-10fd-43b9-b390-c8f48d0a3e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# ----------------- Load dataset and BERT Model for Embeddings -----------------\n",
    "\n",
    "file_path = \"~/books/Cleaned Dataset/Book_Cleaned_Dataset_.xls\"\n",
    "df = pd.read_csv(file_path, encoding=\"utf-8-sig\")\n",
    "# Initialize tokenizer, device\n",
    "bert_model_name = 'asafaya/bert-base-arabic'\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = AutoModel.from_pretrained(bert_model_name)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert_model.to(device)\n",
    "bert_model.eval()\n",
    "\n",
    "# ----------------- Embedding with the use of batching for faster preformence -----------------\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),  # Remove batch dimension\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0)\n",
    "        }\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    last_hidden_state = model_output.last_hidden_state\n",
    "    mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "    summed = torch.sum(last_hidden_state * mask, dim=1)\n",
    "    counts = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "    return summed / counts  # (batch_size, hidden_dim)\n",
    "\n",
    "def convert_to_embeddings(df, column_names, model_name=\"asafaya/bert-base-arabic\", max_length=512, batch_size=32, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    for column_name in column_names:\n",
    "        print(f\"\\nProcessing column: {column_name}\")\n",
    "        \n",
    "        # Ensure all text data is string format\n",
    "        texts = df[column_name].astype(str).tolist()\n",
    "        \n",
    "        # Create dataset and dataloader\n",
    "        dataset = TextDataset(texts, tokenizer, max_length)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        embeddings = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Generating embeddings\"):\n",
    "                # Move input tensors to GPU\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                \n",
    "                # Get model output\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                \n",
    "                # Apply mean pooling\n",
    "                sentence_embeddings = mean_pooling(outputs, attention_mask)\n",
    "                \n",
    "                # Move to CPU and store embeddings\n",
    "                embeddings.append(sentence_embeddings.cpu().numpy())\n",
    "\n",
    "        # Store embeddings in DataFrame\n",
    "        df[f\"{column_name}_embedded\"] = list(np.concatenate(embeddings, axis=0))\n",
    "        print(f\"Completed embedding generation for {column_name}\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ed616166-8a1b-44e2-89c1-9637539bd5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Category\n",
      "0     10000\n",
      "1        10\n",
      "2  10000000\n",
      "   Category\n",
      "0     10000\n",
      "1        10\n",
      "2  10000000\n",
      "  Category_original\n",
      "0  الصحافة والإعلام\n",
      "1   الكتب الإسلامية\n",
      "2     الأسرة والطفل\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------  Decode the catgories -----------------\n",
    "\n",
    "def decode_categories(df):\n",
    "\n",
    "    # Define the category mapping\n",
    "    category_map = {\n",
    "        \"الأدب والخيال\": \"1\",\n",
    "        \"الكتب الإسلامية\": \"10\",\n",
    "        \"الاقتصاد والأعمال\": \"100\",\n",
    "        \"الفلسفة\": \"1000\",\n",
    "        \"الصحافة والإعلام\": \"10000\",\n",
    "        \"الكتب السياسية\": \"100000\",\n",
    "        \"العلوم والطبيعة\": \"1000000\",\n",
    "        \"الأسرة والطفل\": \"10000000\",\n",
    "        \"السير والمذكرات\": \"100000000\",\n",
    "        \"الفنون\": \"1000000000\",\n",
    "        \"التاريخ والجغرافيا\": \"10000000000\",\n",
    "        \"الرياضة والتسلية\": \"100000000000\",\n",
    "        \"الشرع والقانون\": \"1000000000000\"\n",
    "    }\n",
    "    \n",
    "    # Create reversed mapping\n",
    "    reversed_category_map = {v: k for k, v in category_map.items()}\n",
    "    \n",
    "    # Convert category values to string to ensure proper matching\n",
    "    df['Category'] = df['Category'].astype(str)\n",
    "    \n",
    "    # Function to safely map categories\n",
    "    def safe_map_category(x):\n",
    "        if pd.isna(x) or x == 'nan':\n",
    "            return np.nan\n",
    "        \n",
    "        # Convert the input to a simple string of the number\n",
    "        x_str = str(int(x))  # This removes leading zeros and converts to simple number string\n",
    "        \n",
    "        return reversed_category_map.get(x_str, x)\n",
    "    \n",
    "    # Apply the mapping\n",
    "    df['Category_original'] = df['Category'].apply(safe_map_category)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(df[['Category']].head(3))\n",
    "df = decode_categories(df)\n",
    "print(df[['Category']].head(3))\n",
    "print(df[['Category_original']].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3c65f978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    3299.000000\n",
      "mean       99.467717\n",
      "std        95.465457\n",
      "min         5.000000\n",
      "25%        60.000000\n",
      "50%        67.000000\n",
      "75%        78.000000\n",
      "max      1377.000000\n",
      "Name: word_count_Title_Description, dtype: float64\n",
      "Loading model and tokenizer...\n",
      "\n",
      "Processing column: Title_Description\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 104/104 [24:48<00:00, 14.31s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed embedding generation for Title_Description\n"
     ]
    }
   ],
   "source": [
    "# ----------------- Merge Title & Description -----------------\n",
    "# we have merged them for higher accuracy result of the training model\n",
    "df['Title_Description'] = df['Title'] + \" \" + df['Description'] \n",
    "\n",
    "# ----------------- max_length descion making process -----------------\n",
    "\n",
    "# we will decide the max_length based on the following results \n",
    "# Calculate the number of words in each text\n",
    "df['word_count_Title_Description'] = df['Title_Description'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Analyze the distribution\n",
    "print(df['word_count_Title_Description'].describe())\n",
    "df = df.drop(['word_count_Title_Description'], axis=1)\n",
    "\n",
    "\n",
    "#%75 of 'Title_Description' will be covered and 128 will avoid excessive padding for shorter descriptions\n",
    "# and will truncates very long descriptions\n",
    "df = convert_to_embeddings(df, \n",
    "                         column_names=['Title_Description'], \n",
    "                         max_length=128, \n",
    "                         batch_size=32)\n",
    "\n",
    "\n",
    "#Flatten embeddings into separate columns\n",
    "df = pd.concat([df.drop(['Title_Description'], axis=1),\n",
    "                df['Title_Description'].apply(pd.Series),], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a8438b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Title  Author  \\\n",
      "0  التشبيك وميثاق الممارسة في عمل المنظمات الأهلية    2073   \n",
      "\n",
      "                                         Description  Pages  Publication year  \\\n",
      "0  تقرير يوثق أعمال ورشة عمل 1995 عن محاولة صياغة...     40              2003   \n",
      "\n",
      "   Publisher Category  Subcategory  Price Page Range Category_original  \\\n",
      "0        145    10000           65  16.88       0-50  الصحافة والإعلام   \n",
      "\n",
      "                          Title_Description_embedded  \\\n",
      "0  [0.484785, -0.12686853, 0.12688546, -0.4015932...   \n",
      "\n",
      "                                                   0  \n",
      "0  التشبيك وميثاق الممارسة في عمل المنظمات الأهلي...  \n"
     ]
    }
   ],
   "source": [
    "# the end result of the dataset in the training we will use Category_original\n",
    "# in catogory format after decoding to utilize catboost \n",
    "print (df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dc611ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 2.5174652\ttotal: 298ms\tremaining: 2m 28s\n",
      "200:\tlearn: 0.7645502\ttotal: 1m 10s\tremaining: 1m 45s\n",
      "400:\tlearn: 0.4811655\ttotal: 2m 19s\tremaining: 34.4s\n",
      "499:\tlearn: 0.4124898\ttotal: 2m 53s\tremaining: 0us\n",
      "Test Accuracy: 0.6833\n"
     ]
    }
   ],
   "source": [
    "# ----------------- Train CatBoost Classifier -----------------\n",
    "\n",
    "# We Used only the embeddings for Description and title  as features and Category_original as the target\n",
    "X = df['Title_Description_embedded'].apply(np.array).tolist()  # Only use Description embeddings as features\n",
    "X = np.array(X)  # Convert the list to a numpy array\n",
    "\n",
    "# The target variable is 'Category_original'\n",
    "y = df['Category_original']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the CatBoost model\n",
    "catboost_model = CatBoostClassifier(iterations=500,  # Number of trees\n",
    "                          depth=6,  # Depth of each tree\n",
    "                          learning_rate=0.05,  # Learning rate\n",
    "                          loss_function='MultiClass',  # Multi-class classification\n",
    "                          cat_features=[],  # No categorical features in this case\n",
    "                          early_stopping_rounds=5,\n",
    "                          )\n",
    "\n",
    "#  Train the model\n",
    "catboost_model.fit(X_train, y_train, verbose=200)  # Print training progress every 200 iterations\n",
    "\n",
    "#  Make predictions on the test set\n",
    "y_pred = catboost_model.predict(X_test)\n",
    "\n",
    "# Save trained model\n",
    "joblib.dump(catboost_model, \"catboost_classifier.pkl\")\n",
    "\n",
    "#  Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9fd4f919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Category: ['الأدب والخيال']\n"
     ]
    }
   ],
   "source": [
    "# Classify a new description (example)\n",
    "example_description = \"قصص مغامرات للأطفال\"\n",
    "\n",
    "# Tokenize using BERT\n",
    "example_inputs = tokenizer(\n",
    "    example_description, truncation=True, max_length=128, padding='max_length', return_tensors='pt'\n",
    ").to(device)\n",
    "\n",
    "# Generate embedding\n",
    "with torch.no_grad():\n",
    "    outputs = bert_model(example_inputs['input_ids'], attention_mask=example_inputs['attention_mask'])\n",
    "\n",
    "# Mean Pooling\n",
    "mask = example_inputs['attention_mask'].unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()\n",
    "masked_embeddings = outputs.last_hidden_state * mask\n",
    "summed = torch.sum(masked_embeddings, dim=1)\n",
    "counts = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "mean_pooled = summed / counts  # Final sentence embedding\n",
    "\n",
    "# Convert to NumPy (reshape for CatBoost)\n",
    "example_embedding = mean_pooled.cpu().numpy().reshape(1, -1)\n",
    "\n",
    "# Predict category using trained CatBoost model\n",
    "example_category = catboost_model.predict(example_embedding)[0]\n",
    "\n",
    "print(f\"Predicted Category: {example_category}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
