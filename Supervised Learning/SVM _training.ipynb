{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28b91ce9-4e22-475f-a354-e5c6d93f7157",
   "metadata": {},
   "source": [
    "we will decode the Categories to use catboost in its full limits,because after a search we did we found out that CatBoost might misinterpret the binary-encoded values as having ordinal relationships and it will treat them as numerical features rather than categorical features. which means CatBoost won’t apply its specialized handling for categorical data (e.g., Ordered Target Encoding), which could reduce model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e8ec8c",
   "metadata": {},
   "source": [
    "Justification for Choosing SVM\n",
    "\n",
    "Support Vector Machines (SVM) is a well-established model in text classification due to its ability to handle high-dimensional feature spaces. In this project, book descriptions were transformed into numerical feature vectors using TF-IDF (Term Frequency-Inverse Document Frequency) or word embeddings, making SVM a strong candidate.\n",
    "\n",
    "Key reasons for selecting SVM:\n",
    "\n",
    "    1.\tEffective in High-Dimensional Spaces: Text data results in a large feature space, and SVM is designed to handle such cases efficiently.\n",
    "    2.\tStrong Generalization Ability: SVM finds the optimal decision boundary between categories, reducing overfitting.\n",
    "    3.\tWorks Well with Medium-Sized Datasets: With 3,300 rows, deep learning models would require significantly more data, whereas SVM remains effective with this dataset size.\n",
    "    4.\tProven Success in Text Categorization: SVM has been widely used in spam detection, sentiment analysis, and document classification\n",
    ".\n",
    "\n",
    "Reference for SVM\n",
    "\n",
    "A foundational study on SVM for text classification is:\n",
    "Joachims, T. & Universität Dortmund. (1998). Text Categorization with Support Vector Machines: Learning with Many Relevant Features. https://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf\n",
    "\n",
    "This paper provides theoretical and empirical evidence that SVM is well-suited for categorizing text into distinct classes. It demonstrates that SVM outperforms traditional models such as Naïve Bayes in document classification tasks.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "958c8519-8ca2-4877-bcfd-a4c983e9ff36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catboost in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.2.7)\n",
      "Requirement already satisfied: graphviz in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from catboost) (0.20.3)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from catboost) (3.10.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from catboost) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.24 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from catboost) (2.2.3)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from catboost) (1.15.2)\n",
      "Requirement already satisfied: plotly in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from catboost) (6.0.0)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from catboost) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas>=0.24->catboost) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas>=0.24->catboost) (2025.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->catboost) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->catboost) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->catboost) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->catboost) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->catboost) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->catboost) (3.2.1)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from plotly->catboost) (1.27.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: openpyxl in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.1.5)\n",
      "Requirement already satisfied: xlrd in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.0.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: et-xmlfile in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.49.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (2025.1.31)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.26.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install catboost\n",
    "%pip install pandas openpyxl xlrd\n",
    "%pip install torch\n",
    "%pip install transformers\n",
    "%pip install numpy\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2a45d72-10fd-43b9-b390-c8f48d0a3e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"/Users/batoolalfouzan/Desktop/books-main/Cleaned Dataset/Book_Cleaned_Dataset_.xls\"\n",
    "df = pd.read_csv(file_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze()\n",
    "        }\n",
    "\n",
    "def convert_to_embeddings(df, column_names, max_length=512, batch_size=32, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "  \n",
    "    # Load model and tokenizer once\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained('asafaya/bert-base-arabic')\n",
    "    model = AutoModel.from_pretrained('asafaya/bert-base-arabic')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    for column_name in column_names:\n",
    "        print(f\"\\nProcessing column: {column_name}\")\n",
    "        \n",
    "        # Create dataset and dataloader\n",
    "        texts = df[column_name].tolist()\n",
    "        dataset = TextDataset(texts, tokenizer, max_length)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # Process batches\n",
    "        embeddings = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Generating embeddings\"):\n",
    "                # Move batch to device\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                \n",
    "                # Generate embeddings\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                \n",
    "                # Compute mean pooling\n",
    "                mask = attention_mask.unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()\n",
    "                masked_embeddings = outputs.last_hidden_state * mask\n",
    "                summed = torch.sum(masked_embeddings, 1)\n",
    "                counts = torch.clamp(mask.sum(1), min=1e-9)\n",
    "                mean_pooled = summed / counts\n",
    "                \n",
    "                # Move to CPU and convert to numpy\n",
    "                embeddings.append(mean_pooled.cpu().numpy())\n",
    "        \n",
    "        # Concatenate all batches\n",
    "        all_embeddings = np.concatenate(embeddings, axis=0)\n",
    "        \n",
    "        # Store embeddings in DataFrame\n",
    "        df[f\"{column_name}_embedded\"] = list(all_embeddings)\n",
    "        \n",
    "        print(f\"Completed embedding generation for {column_name}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed616166-8a1b-44e2-89c1-9637539bd5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Category\n",
      "0     10000\n",
      "1        10\n",
      "2  10000000\n",
      "   Category\n",
      "0     10000\n",
      "1        10\n",
      "2  10000000\n",
      "  Category_original\n",
      "0  الصحافة والإعلام\n",
      "1   الكتب الإسلامية\n",
      "2     الأسرة والطفل\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def decode_categories(df):\n",
    "\n",
    "    # Define the category mapping\n",
    "    category_map = {\n",
    "        \"الأدب والخيال\": \"1\",\n",
    "        \"الكتب الإسلامية\": \"10\",\n",
    "        \"الاقتصاد والأعمال\": \"100\",\n",
    "        \"الفلسفة\": \"1000\",\n",
    "        \"الصحافة والإعلام\": \"10000\",\n",
    "        \"الكتب السياسية\": \"100000\",\n",
    "        \"العلوم والطبيعة\": \"1000000\",\n",
    "        \"الأسرة والطفل\": \"10000000\",\n",
    "        \"السير والمذكرات\": \"100000000\",\n",
    "        \"الفنون\": \"1000000000\",\n",
    "        \"التاريخ والجغرافيا\": \"10000000000\",\n",
    "        \"الرياضة والتسلية\": \"100000000000\",\n",
    "        \"الشرع والقانون\": \"1000000000000\"\n",
    "    }\n",
    "    \n",
    "    # Create reversed mapping\n",
    "    reversed_category_map = {v: k for k, v in category_map.items()}\n",
    "    \n",
    "    # Convert category values to string to ensure proper matching\n",
    "    df['Category'] = df['Category'].astype(str)\n",
    "    \n",
    "    # Function to safely map categories\n",
    "    def safe_map_category(x):\n",
    "        if pd.isna(x) or x == 'nan':\n",
    "            return np.nan\n",
    "        \n",
    "        # Convert the input to a simple string of the number\n",
    "        x_str = str(int(x))  # This removes leading zeros and converts to simple number string\n",
    "        \n",
    "        return reversed_category_map.get(x_str, x)\n",
    "    \n",
    "    # Apply the mapping\n",
    "    df['Category_original'] = df['Category'].apply(safe_map_category)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(df[['Category']].head(3))\n",
    "df = decode_categories(df)\n",
    "print(df[['Category']].head(3))\n",
    "print(df[['Category_original']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa686ccd-2f19-4eef-acc2-3033a43a31aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Subcategory Subcategory_original\n",
      "0           65   الندوات والمؤتمرات\n",
      "1           54        القرآن وعلومه\n",
      "2           71          شؤون المرأة\n",
      "3           74           علم النبات\n",
      "4           68          تاريخ الأدب\n"
     ]
    }
   ],
   "source": [
    "# Load the encoder later\n",
    "with open(\"label_encoder_Subcategory.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "    \n",
    "# Decode the 'Subcategory' column back to original values\n",
    "df['Subcategory_original'] = label_encoder.inverse_transform(df['Subcategory'])\n",
    "\n",
    "# Display a sample of the reversed data\n",
    "print(df[['Subcategory', 'Subcategory_original']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c65f978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    3299.000000\n",
      "mean       94.642316\n",
      "std        95.139226\n",
      "min         2.000000\n",
      "25%        56.000000\n",
      "50%        63.000000\n",
      "75%        72.000000\n",
      "max      1374.000000\n",
      "Name: word_count_Description, dtype: float64\n",
      "count    3299.000000\n",
      "mean        4.825402\n",
      "std         2.597022\n",
      "min         1.000000\n",
      "25%         3.000000\n",
      "50%         4.000000\n",
      "75%         6.000000\n",
      "max        20.000000\n",
      "Name: word_count_Title, dtype: float64\n",
      "Loading model and tokenizer...\n",
      "\n",
      "Processing column: Title\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 104/104 [00:18<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed embedding generation for Title\n",
      "Loading model and tokenizer...\n",
      "\n",
      "Processing column: Description\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 104/104 [01:26<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed embedding generation for Description\n"
     ]
    }
   ],
   "source": [
    "# we will decide the max_length based on the following results \n",
    "# Calculate the number of words in each text\n",
    "df['word_count_Description'] = df['Description'].apply(lambda x: len(x.split()))\n",
    "df['word_count_Title'] = df['Title'].apply(lambda x: len(x.split()))\n",
    "# Analyze the distribution\n",
    "print(df['word_count_Description'].describe())\n",
    "print(df['word_count_Title'].describe())\n",
    "df.drop(['word_count_Title', 'word_count_Description'], axis=1)\n",
    "\n",
    "df = convert_to_embeddings(df, \n",
    "                         column_names=['Title'], \n",
    "                         max_length=20, \n",
    "                         batch_size=32)\n",
    "#%75 of descriptions will be covered and 128 will avoid excessive padding for shorter descriptions\n",
    "# and will truncates very long descriptions\n",
    "df = convert_to_embeddings(df, \n",
    "                         column_names=[\"Description\"], \n",
    "                         max_length=128, \n",
    "                         batch_size=32)\n",
    "\n",
    "#Flatten embeddings into separate columns\n",
    "df = pd.concat([df.drop(['Title', 'Description'], axis=1),\n",
    "                df['Title'].apply(pd.Series),\n",
    "                df['Description'].apply(pd.Series)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8438b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Author  Pages  Publication year  Publisher Category  Subcategory  Price  \\\n",
      "0    2073     40              2003        145    10000           65  16.88   \n",
      "\n",
      "  Page Range Category_original Subcategory_original  word_count_Description  \\\n",
      "0       0-50  الصحافة والإعلام   الندوات والمؤتمرات                      71   \n",
      "\n",
      "   word_count_Title                                     Title_embedded  \\\n",
      "0                 7  [0.64062077, -0.59859043, 0.09329636, -0.49279...   \n",
      "\n",
      "                                Description_embedded  \\\n",
      "0  [0.4787133, -0.15091261, 0.24697617, -0.482517...   \n",
      "\n",
      "                                                 0  \\\n",
      "0  التشبيك وميثاق الممارسة في عمل المنظمات الأهلية   \n",
      "\n",
      "                                                   0  \n",
      "0  تقرير يوثق أعمال ورشة عمل 1995 عن محاولة صياغة...  \n"
     ]
    }
   ],
   "source": [
    "# the end result of the dataset in the training we will use Subcategory_original, Category_original\n",
    "# in catogory format after decoding to utilize catboost \n",
    "print (df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8b89292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.56060606 0.59659091 0.59659091 0.61174242 0.57874763]\n",
      "Mean CV score: 0.5888555862227589\n",
      "Training Accuracy: 0.9913\n",
      "Test Accuracy: 0.6212\n",
      "Predicted Category: ['الأسرة والطفل']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC \n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the tokenizer and model for Arabic BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\n",
    "bert_model = AutoModel.from_pretrained(\"asafaya/bert-base-arabic\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1️. Prepare the dataset for SVM\n",
    "X = df['Description_embedded'].apply(np.array).tolist()   # Convert embeddings column to lists\n",
    "X = np.array(X)  # Convert the list to a numpy array\n",
    "\n",
    "# The target variable (category labels)\n",
    "y = df['Category_original']\n",
    "\n",
    "# 2. Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Initialize the SVM model\n",
    "svm_model = SVC(kernel=\"linear\", probability=True, random_state=42)\n",
    "\n",
    "# 4. Train the model\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# 4. **Perform Cross-Validation**\n",
    "cross_val_scores = cross_val_score(svm_model, X_train, y_train, cv=5)  # 5-fold cross-validation\n",
    "print(f\"Cross-validation scores: {cross_val_scores}\")\n",
    "print(f\"Mean CV score: {cross_val_scores.mean()}\")\n",
    "\n",
    "# 5. Calculate Training Accuracy\n",
    "y_train_pred = svm_model.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "# 6. Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "pd.DataFrame(y_pred, columns=['Predictions']).to_csv(\"y_pred_svm.csv\", index=False)\n",
    "\n",
    "# 7. Calculate Test Accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# 9. Classify a new description (example)\n",
    "example_description = \"طفل\"  # Example new description\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Convert the new description into model-compatible input using the tokenizer\n",
    "example_inputs = tokenizer(example_description, truncation=True, max_length=128, padding='max_length', return_tensors='pt').to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = bert_model(example_inputs['input_ids'], attention_mask=example_inputs['attention_mask'])\n",
    "    mask = example_inputs['attention_mask'].unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()\n",
    "    masked_embeddings = outputs.last_hidden_state * mask\n",
    "    summed = torch.sum(masked_embeddings, dim=1)\n",
    "    counts = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "    mean_pooled = summed / counts  # final embedding\n",
    "\n",
    "# Convert embedding to NumPy for SVM prediction\n",
    "example_embedding = mean_pooled.cpu().numpy()\n",
    "\n",
    "# 10. Predict the category using the trained SVM model\n",
    "example_category = svm_model.predict(example_embedding)\n",
    "print(f\"Predicted Category: {example_category}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
