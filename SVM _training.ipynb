{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28b91ce9-4e22-475f-a354-e5c6d93f7157",
   "metadata": {},
   "source": [
    "we will decode the Categories and subcategories to use catboost in its full limits,because after a search we did we found out that CatBoost might misinterpret the binary-encoded values as having ordinal relationships and it will treat them as numerical features rather than categorical features. which means CatBoost won’t apply its specialized handling for categorical data (e.g., Ordered Target Encoding), which could reduce model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "958c8519-8ca2-4877-bcfd-a4c983e9ff36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catboost in c:\\users\\raghad\\anaconda3\\lib\\site-packages (1.2.7)\n",
      "Requirement already satisfied: graphviz in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from catboost) (0.20.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from catboost) (3.9.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from catboost) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.24 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from catboost) (2.2.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from catboost) (1.13.1)\n",
      "Requirement already satisfied: plotly in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from catboost) (5.24.1)\n",
      "Requirement already satisfied: six in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from pandas>=0.24->catboost) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from pandas>=0.24->catboost) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (3.1.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from plotly->catboost) (8.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~rompt-toolkit (c:\\Users\\Raghad\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~rompt-toolkit (c:\\Users\\Raghad\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~rompt-toolkit (c:\\Users\\Raghad\\anaconda3\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\raghad\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\raghad\\anaconda3\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: xlrd in c:\\users\\raghad\\anaconda3\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~rompt-toolkit (c:\\Users\\Raghad\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~rompt-toolkit (c:\\Users\\Raghad\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~rompt-toolkit (c:\\Users\\Raghad\\anaconda3\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\raghad\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~rompt-toolkit (c:\\Users\\Raghad\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~rompt-toolkit (c:\\Users\\Raghad\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~rompt-toolkit (c:\\Users\\Raghad\\anaconda3\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\raghad\\anaconda3\\lib\\site-packages (4.48.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~rompt-toolkit (c:\\Users\\Raghad\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~rompt-toolkit (c:\\Users\\Raghad\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~rompt-toolkit (c:\\Users\\Raghad\\anaconda3\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\raghad\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~rompt-toolkit (c:\\Users\\Raghad\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~rompt-toolkit (c:\\Users\\Raghad\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~rompt-toolkit (c:\\Users\\Raghad\\anaconda3\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\raghad\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\raghad\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~rompt-toolkit (c:\\Users\\Raghad\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~rompt-toolkit (c:\\Users\\Raghad\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~rompt-toolkit (c:\\Users\\Raghad\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install catboost\n",
    "%pip install pandas openpyxl xlrd\n",
    "%pip install torch\n",
    "%pip install transformers\n",
    "%pip install numpy\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2a45d72-10fd-43b9-b390-c8f48d0a3e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"C:\\Users\\Raghad\\Documents\\books\\Cleaned Dataset\\Book_Cleaned_Dataset_.xls\"\n",
    "df = pd.read_csv(file_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze()\n",
    "        }\n",
    "\n",
    "def convert_to_embeddings(df, column_names, max_length=512, batch_size=32, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "  \n",
    "    # Load model and tokenizer once\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained('asafaya/bert-base-arabic')\n",
    "    model = AutoModel.from_pretrained('asafaya/bert-base-arabic')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    for column_name in column_names:\n",
    "        print(f\"\\nProcessing column: {column_name}\")\n",
    "        \n",
    "        # Create dataset and dataloader\n",
    "        texts = df[column_name].tolist()\n",
    "        dataset = TextDataset(texts, tokenizer, max_length)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # Process batches\n",
    "        embeddings = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Generating embeddings\"):\n",
    "                # Move batch to device\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                \n",
    "                # Generate embeddings\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                \n",
    "                # Compute mean pooling\n",
    "                mask = attention_mask.unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()\n",
    "                masked_embeddings = outputs.last_hidden_state * mask\n",
    "                summed = torch.sum(masked_embeddings, 1)\n",
    "                counts = torch.clamp(mask.sum(1), min=1e-9)\n",
    "                mean_pooled = summed / counts\n",
    "                \n",
    "                # Move to CPU and convert to numpy\n",
    "                embeddings.append(mean_pooled.cpu().numpy())\n",
    "        \n",
    "        # Concatenate all batches\n",
    "        all_embeddings = np.concatenate(embeddings, axis=0)\n",
    "        \n",
    "        # Store embeddings in DataFrame\n",
    "        df[f\"{column_name}_embedded\"] = list(all_embeddings)\n",
    "        \n",
    "        print(f\"Completed embedding generation for {column_name}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed616166-8a1b-44e2-89c1-9637539bd5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Category\n",
      "0     10000\n",
      "1        10\n",
      "2  10000000\n",
      "   Category\n",
      "0     10000\n",
      "1        10\n",
      "2  10000000\n",
      "  Category_original\n",
      "0  الصحافة والإعلام\n",
      "1   الكتب الإسلامية\n",
      "2     الأسرة والطفل\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def decode_categories(df):\n",
    "\n",
    "    # Define the category mapping\n",
    "    category_map = {\n",
    "        \"الأدب والخيال\": \"1\",\n",
    "        \"الكتب الإسلامية\": \"10\",\n",
    "        \"الاقتصاد والأعمال\": \"100\",\n",
    "        \"الفلسفة\": \"1000\",\n",
    "        \"الصحافة والإعلام\": \"10000\",\n",
    "        \"الكتب السياسية\": \"100000\",\n",
    "        \"العلوم والطبيعة\": \"1000000\",\n",
    "        \"الأسرة والطفل\": \"10000000\",\n",
    "        \"السير والمذكرات\": \"100000000\",\n",
    "        \"الفنون\": \"1000000000\",\n",
    "        \"التاريخ والجغرافيا\": \"10000000000\",\n",
    "        \"الرياضة والتسلية\": \"100000000000\",\n",
    "        \"الشرع والقانون\": \"1000000000000\"\n",
    "    }\n",
    "    \n",
    "    # Create reversed mapping\n",
    "    reversed_category_map = {v: k for k, v in category_map.items()}\n",
    "    \n",
    "    # Convert category values to string to ensure proper matching\n",
    "    df['Category'] = df['Category'].astype(str)\n",
    "    \n",
    "    # Function to safely map categories\n",
    "    def safe_map_category(x):\n",
    "        if pd.isna(x) or x == 'nan':\n",
    "            return np.nan\n",
    "        \n",
    "        # Convert the input to a simple string of the number\n",
    "        x_str = str(int(x))  # This removes leading zeros and converts to simple number string\n",
    "        \n",
    "        return reversed_category_map.get(x_str, x)\n",
    "    \n",
    "    # Apply the mapping\n",
    "    df['Category_original'] = df['Category'].apply(safe_map_category)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(df[['Category']].head(3))\n",
    "df = decode_categories(df)\n",
    "print(df[['Category']].head(3))\n",
    "print(df[['Category_original']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa686ccd-2f19-4eef-acc2-3033a43a31aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Subcategory Subcategory_original\n",
      "0           65   الندوات والمؤتمرات\n",
      "1           54        القرآن وعلومه\n",
      "2           71          شؤون المرأة\n",
      "3           74           علم النبات\n",
      "4           68          تاريخ الأدب\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Raghad\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.6.1 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the encoder later\n",
    "with open(\"label_encoder_Subcategory.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "    \n",
    "# Decode the 'Subcategory' column back to original values\n",
    "df['Subcategory_original'] = label_encoder.inverse_transform(df['Subcategory'])\n",
    "\n",
    "# Display a sample of the reversed data\n",
    "print(df[['Subcategory', 'Subcategory_original']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c65f978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    3299.000000\n",
      "mean       94.642316\n",
      "std        95.139226\n",
      "min         2.000000\n",
      "25%        56.000000\n",
      "50%        63.000000\n",
      "75%        72.000000\n",
      "max      1374.000000\n",
      "Name: word_count_Description, dtype: float64\n",
      "count    3299.000000\n",
      "mean        4.825402\n",
      "std         2.597022\n",
      "min         1.000000\n",
      "25%         3.000000\n",
      "50%         4.000000\n",
      "75%         6.000000\n",
      "max        20.000000\n",
      "Name: word_count_Title, dtype: float64\n",
      "Loading model and tokenizer...\n",
      "\n",
      "Processing column: Title\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 104/104 [01:32<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed embedding generation for Title\n",
      "Loading model and tokenizer...\n",
      "\n",
      "Processing column: Description\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 104/104 [08:42<00:00,  5.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed embedding generation for Description\n"
     ]
    }
   ],
   "source": [
    "# we will decide the max_length based on the following results \n",
    "# Calculate the number of words in each text\n",
    "df['word_count_Description'] = df['Description'].apply(lambda x: len(x.split()))\n",
    "df['word_count_Title'] = df['Title'].apply(lambda x: len(x.split()))\n",
    "# Analyze the distribution\n",
    "print(df['word_count_Description'].describe())\n",
    "print(df['word_count_Title'].describe())\n",
    "df.drop(['word_count_Title', 'word_count_Description'], axis=1)\n",
    "\n",
    "df = convert_to_embeddings(df, \n",
    "                         column_names=['Title'], \n",
    "                         max_length=20, \n",
    "                         batch_size=32)\n",
    "#%75 of descriptions will be covered and 128 will avoid excessive padding for shorter descriptions\n",
    "# and will truncates very long descriptions\n",
    "df = convert_to_embeddings(df, \n",
    "                         column_names=[\"Description\"], \n",
    "                         max_length=128, \n",
    "                         batch_size=32)\n",
    "\n",
    "#Flatten embeddings into separate columns\n",
    "df = pd.concat([df.drop(['Title', 'Description'], axis=1),\n",
    "                df['Title'].apply(pd.Series),\n",
    "                df['Description'].apply(pd.Series)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8438b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Author  Pages  Publication year  Publisher Category  Subcategory  Price  \\\n",
      "0    2073     40              2003        145    10000           65  16.88   \n",
      "\n",
      "  Page Range Category_original Subcategory_original  word_count_Description  \\\n",
      "0       0-50  الصحافة والإعلام   الندوات والمؤتمرات                      71   \n",
      "\n",
      "   word_count_Title                                     Title_embedded  \\\n",
      "0                 7  [0.6406202, -0.59858966, 0.0932962, -0.492798,...   \n",
      "\n",
      "                                Description_embedded  \\\n",
      "0  [0.47871357, -0.15091306, 0.24697728, -0.48251...   \n",
      "\n",
      "                                                 0  \\\n",
      "0  التشبيك وميثاق الممارسة في عمل المنظمات الأهلية   \n",
      "\n",
      "                                                   0  \n",
      "0  تقرير يوثق أعمال ورشة عمل 1995 عن محاولة صياغة...  \n"
     ]
    }
   ],
   "source": [
    "# the end result of the dataset in the training we will use Subcategory_original, Category_original\n",
    "# in catogory format after decoding to utilize catboost \n",
    "print (df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc611ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6212\n",
      "Predicted Category: ['الأسرة والطفل']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "# Load the tokenizer and model for Arabic BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\n",
    "bert_model = AutoModel.from_pretrained(\"asafaya/bert-base-arabic\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1️. Prepare the dataset for SVM\n",
    "X = df['Description_embedded'].apply(np.array).tolist()   # Convert embeddings column to lists\n",
    "X = np.array(X)  # Convert the list to a numpy array\n",
    "\n",
    "# The target variable (category labels)\n",
    "y = df['Category_original']\n",
    "\n",
    "# 2. Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Initialize the SVM model\n",
    "svm_model = SVC(kernel=\"linear\", probability=True, random_state=42)\n",
    "\n",
    "# 4. Train the model\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# 5. Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# 6. Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# 7️.Classify a new description (example)\n",
    "example_description = \"طفل\"  # وصف جديد\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Convert the new description into model-compatible input using the tokenizer\n",
    "example_inputs = tokenizer(example_description, truncation=True, max_length=128, padding='max_length', return_tensors='pt').to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = bert_model(example_inputs['input_ids'], attention_mask=example_inputs['attention_mask'])\n",
    "    mask = example_inputs['attention_mask'].unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()\n",
    "    masked_embeddings = outputs.last_hidden_state * mask\n",
    "    summed = torch.sum(masked_embeddings, dim=1)\n",
    "    counts = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "    mean_pooled = summed / counts  # التضمين النهائي\n",
    "\n",
    "# Convert embedding to NumPy for SVM prediction\n",
    "example_embedding = mean_pooled.cpu().numpy()\n",
    "\n",
    "# 8️.Predict the category using the trained SVM mode\n",
    "example_category = svm_model.predict(example_embedding)\n",
    "print(f\"Predicted Category: {example_category}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
