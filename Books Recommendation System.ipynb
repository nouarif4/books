{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c327256-ce49-4432-b747-ef1312b5a8cf",
   "metadata": {},
   "source": [
    "Books Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fcc811-3b90-4559-8e62-05f2dda80d0d",
   "metadata": {},
   "source": [
    "Project Motivation\n",
    "The Arabic book market is a huge market with many readers worldwide. This poses a challenge of finding the right book to read especially since there are limited Arabic-specific recommendation systems available. The goal of this project is to help users discover their next read, based on their interests, preferences, and reading history by building an intelligent recommendation system utilizing the Jamalon Arabic Books Dataset.\n",
    "\n",
    "students' names: رغد المطيري | شوق القريشي | هيفاء السديري | بتول الفوزان | نوره العريفي |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a029347-1b3f-49fd-89d4-c7f073c4cf47",
   "metadata": {},
   "source": [
    "The goal of the dataset:\n",
    "\n",
    "The primary goal of using the Jamalon Arabic Books Dataset is to help us develop a robust, AI-driven, personalized recommendation system for Arabic books, enabling more efficient book categorization and enhancing overall user experience. By leveraging this dataset which includes rich metadata such as titles, authors, and genres, the system will provide personalized suggestions and explanations by integrating machine learning and generative AI, classify and categorize content efficiently, and process user inputs\n",
    "\n",
    "The goal of this project is to develop a recommendation system for Arabic books available on Jamalon, an online bookstore. The objective is to suggest books to users based on attributes such as genre, price, and ratings, along with personal preferences.\n",
    "\n",
    "In Phase 1, we focused on understanding the problem, exploring the dataset, and performing initial data preprocessing. This included handling missing values, encoding categorical features, and visualizing the dataset to understand the relationships between key attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e76a11-4580-4653-8a75-e381963c9551",
   "metadata": {},
   "source": [
    "The source of the dataset:\n",
    "We are using the Jamalon Arabic Books Dataset, sourced from [Kaggle - Jamalon Arabic Books Dataset](https://www.kaggle.com/datasets/dareenalharthi/jamalon-arabic-books-dataset?resource=download)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46bad5a-a474-4f35-b0dd-1a33b0c63f2c",
   "metadata": {},
   "source": [
    "General information:\n",
    "In the Jamalon Arabic Books Dataset, each row represents a book. The dataset consists of 11 columns (variables) and approximately 8980 observations (books).  \n",
    "\n",
    "Dataset Variables:\n",
    "- Unique ID: A unique identifier for each book (Numerical).\n",
    "- Title: The name of the book (Text).\n",
    "- Author: The author's name (Text).\n",
    "- Description: A brief description of the book (Text)\n",
    "- Pages: The total number of pages in the book (Numerical)\n",
    "- Publication Year: The year the book was published (Numerical)\n",
    "- Publisher: The name of the publisher (Categorical).\n",
    "- Cover: The cover type, such as Paperback or Hardcover (Categorical).\n",
    "- Category: The main category of the book (e.g., Literature, Islamic Books) (Categorical).\n",
    "- Subcategory: A more specific classification under each category (Categorical).\n",
    "- Price: The price of the book (Numerical).\n",
    "\n",
    "The Category and Subcategory columns act as classification labels, organizing books into different genres. These labels are useful for building the recommendation model based on user preferences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7357fd52-152a-438a-b683-fa6d0a8dff33",
   "metadata": {},
   "source": [
    "Summary of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c895fd9-3fe0-4c42-9af9-853a805dc347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Importing Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np \n",
    "from langdetect import detect\n",
    "import re\n",
    "from IPython.display import display\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897b370b-e9ac-4f8e-abd4-9dc4d9a6470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Loading the Dataset\n",
    "file_path = r'C:\\Users\\Raghad\\Downloads\\BooksDB\\jamalon dataset.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8299fbc6-ff52-40a8-8775-f8e7fce37406",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10) \n",
    "#Sample of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72296ad7-66c3-44b9-bc20-bf9a3d22145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values per column\n",
    "missing_values = df.isnull().sum().reset_index()\n",
    "missing_values.columns = ['Column', 'Missing Count']\n",
    "missing_values['Missing Percentage'] = (missing_values['Missing Count'] / len(df)) * 100\n",
    "\n",
    "# Display as a table\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6b7157-743b-41dc-9a81-4a01032bcddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=missing_values['Column'], y=missing_values['Missing Count'], palette=\"viridis\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Count of Missing Values\")\n",
    "plt.title(\"Missing Values per Column\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c724a13d-e68d-439e-a453-68050c048aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count, Mean, Standard, Minimum, Maximum, and Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33deced9-d4e7-4687-857c-210be35a9c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics for numerical columns only\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "summary_stats = numeric_df.describe().T  # Summary statistics\n",
    "summary_stats['Variance'] = numeric_df.var()  # Compute variance only for numerical columns\n",
    "\n",
    "summary_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec2a10c-5b6e-4c38-8f9d-ce52506e6c3d",
   "metadata": {},
   "source": [
    "The following preprocessing steps were performed on the dataset:\n",
    "\n",
    "1- Handling Missing Values: We checked for any null values in the dataset and deleted the rows containing them. Removing rows with missing data helps maintain data integrity and ensures that analysis and models are based on complete information.\n",
    "\n",
    "2- Duplicate Detection and Removal:We identified duplicate rows and applied a strategy Deduplication Strategy: -For duplicates, the entry with the lowest non-zero price is kept. -If prices are all zero, the most recent publication year is prioritized. -If publication years are the same, the entry with the longest description is retained. to retain the most relevant entry. Removing duplicates avoids data redundancy, improves data quality, and ensures accurate analysis.\n",
    "\n",
    "3- Unnecessary Column Removal: We removed columns like Unnamed: 0 and Cover as they were not needed for analysis. The Unnamed: 0 column was removed because it simply numbered the rows, which is unnecessary since the dataset already has automatic indexing. The Cover column was removed because it only indicated the type of book cover (e.g., paperback, hardcover, electronic), which was not relevant to our analysis.\n",
    "\n",
    "4- Language Filtering:We detected English titles using regular expressions (regex) and removed them from the dataset. Since the focus is on Arabic books, removing English titles ensures the dataset is relevant to the project’s objectives.\n",
    "\n",
    "5- Category Mapping:We mapped book categories to binary-like codes (e.g., \"الأدب والخيال\" → 0000000000001). This mapping standardizes categories, making them easier to process in machine learning models.\n",
    "\n",
    "6- Encoding Categorical Features:We applied Label Encoding to the Author, Publisher, and Subcategory columns to convert text data into numerical values. Machine learning models require numerical input; encoding categorical features ensures compatibility with these models.\n",
    "\n",
    "7- Discretization:We discretized the Pages column into bins (e.g., 0–50, 50–100, etc.) to categorize books based on page ranges. Discretization helps in analyzing trends across different ranges and simplifies complex numerical data.\n",
    "\n",
    "8- Text Cleaning (Titles & Descriptions):We cleaned the text data by removing Arabic diacritics (Tashkeel), special characters, punctuation marks, and extra spaces. Cleaning text data improves consistency and quality, which is especially important for text analysis and natural language processing (NLP) tasks in phase 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da3e02b-8f1b-43ba-81b9-72d8d1787a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust display settings for better formatting\n",
    "pd.set_option('display.max_columns', None)  # Display all columns\n",
    "pd.set_option('display.max_colwidth', None) # Display full content in each cell\n",
    "pd.set_option('display.expand_frame_repr', False) # Prevent line breaks in tables\n",
    "pd.set_option('display.width', 1000)       # Adjust the table width\n",
    "\n",
    "\n",
    "# Check for duplicates\n",
    "def print_duplicate_stats(df, column):\n",
    "    # Count the occurrences of each value in the specified column\n",
    "    duplicates = df[column].value_counts()\n",
    "\n",
    "    # Print basic duplicate analysis statistics\n",
    "    print(f\"\\nDuplicate analysis for {column}:\")\n",
    "    print(f\"Total rows: {len(df)}\")  # Total number of rows in the dataframe\n",
    "    print(f\"Unique values: {df[column].nunique()}\")  # Number of unique values in the specified column\n",
    "    print(f\"Number of duplicated values: {len(df[df[column].duplicated()])}\")  # Number of duplicated values\n",
    "\n",
    "\n",
    "    # If there are any duplicated values, display more details\n",
    "    if len(duplicates[duplicates > 1]) > 0:\n",
    "        print(\"\\nMost common duplicates:\")\n",
    "        # Display the top duplicate values in a neat table format\n",
    "        display(duplicates[duplicates > 1].head().to_frame('Count').reset_index().rename(columns={'index': column}))\n",
    "\n",
    "        # Show detailed information for some duplicated entries\n",
    "        print(\"\\nSample of duplicated entries (first duplicates):\")\n",
    "        for title in duplicates[duplicates > 1].head(1).index:\n",
    "            print(f\"\\nAll entries for title: {title}\")\n",
    "            \n",
    "            # Display detailed data for each duplicated title with a clean table style\n",
    "            duplicated_data = df[df[column] == title][['Title', 'Author', 'Publisher', 'Publication year', 'Price']]\n",
    "            styled_table = duplicated_data.style.set_table_styles(\n",
    "                [{'selector': 'th', 'props': [('background-color', '#f7f7f7'), ('font-weight', 'bold')]},\n",
    "                 {'selector': 'td', 'props': [('text-align', 'center')]}]\n",
    "            ).set_properties(**{'border': '1px solid black', 'padding': '8px'})\n",
    "            \n",
    "            display(styled_table)  # Display the table with better formatting\n",
    "\n",
    "# Check for duplicated rows in the entire dataframe\n",
    "print(\"Check for duplicates:\\t\" + str(df.duplicated().sum()))\n",
    "\n",
    "# Perform duplicate analysis for the 'Title' column\n",
    "print_duplicate_stats(df, 'Title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60837ce7-fcfe-416c-8929-445fcb22f394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts and unique values per column\n",
    "for i in df.columns:\n",
    "    print(i+\"column\")\n",
    "    print(df[i].value_counts())\n",
    "print(\"Unique values per column:\\n\", df.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1050f76f-4b19-49d1-a4a2-958f28edd20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics of pages, publication year, price\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3eb42b-1ac9-4466-9034-452776ccaf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Data Cleaning\n",
    "clean_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d4dfa-3348-45c3-bd26-ecc7b811bbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect English titles\n",
    "def is_english(text):\n",
    "    try:\n",
    "        english_pattern = re.compile(r'[a-zA-Z]')\n",
    "        return bool(english_pattern.search(str(text)))\n",
    "    except:\n",
    "        return False\n",
    "        \n",
    "# Function to print removal stats\n",
    "def print_removal_stats(df_before, df_after, step_name):\n",
    "    rows_removed = len(df_before) - len(df_after)\n",
    "    print(f\"\\n{step_name}:\")\n",
    "    print(f\"Rows removed: {rows_removed}\")\n",
    "    print(f\"Rows remaining: {len(df_after)}\")\n",
    "    if rows_removed > 0:\n",
    "        removed_df = df_before[~df_before.index.isin(df_after.index)]\n",
    "        print(removed_df['Title'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8898d175-8964-46db-a45f-8124e8e13944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to choose which duplicate to keep\n",
    "def choose_best_duplicate(group):\n",
    "    if len(group) == 1:\n",
    "        return group.iloc[0]  # No duplicates, return as is\n",
    "\n",
    "    # Keep the one with the lowest non-zero price\n",
    "    non_zero_prices = group[group['Price'] > 0]\n",
    "    if not non_zero_prices.empty:\n",
    "        return non_zero_prices.sort_values(by='Price').iloc[0]\n",
    "\n",
    "    # If all prices are zero, keep the most recent publication\n",
    "    if group['Publication year'].nunique() > 1:\n",
    "        return group.sort_values(by='Publication year', ascending=False).iloc[0]\n",
    "\n",
    "    # If publication years are the same, keep the longest description\n",
    "    group['Description_Length'] = group['Description'].fillna('').apply(len)\n",
    "    return group.sort_values(by='Description_Length', ascending=False).iloc[0]\n",
    "\n",
    "# Remove duplicate entries while keeping the best version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dae768-e8ac-46e4-a7ea-9a6ce00af3ec",
   "metadata": {},
   "source": [
    "The columns Unnamed: 0 and Cover were removed because they do not provide any meaningful information relevant to the book recommendation system.\n",
    "Unnamed: 0 is an automatically generated index column that adds no analytical value,\n",
    "while Cover does not contribute to the recommendation process based on attributes like Description, Category.\n",
    "Removing these unnecessary columns simplifies the dataset, improves data processing efficiency, and keeps the focus on relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11d36c1-d3bc-46cb-ab8b-03749bfbc342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing unnecessary columns\n",
    "#Drop unnamed and Cover columns\n",
    "clean_df = clean_df.drop(columns=['Unnamed: 0', 'Cover'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b17f599-ecb7-4111-8301-698a32a6ac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate entries while keeping the best version\n",
    "# This process ensures that for books with the same title and same publisher, we retain the most relevant entry\n",
    "print(\"\\nRemoving duplicates...\")\n",
    "df_before = clean_df.copy()  # Create a copy of the dataset to compare before and after removal\n",
    "\n",
    "# Group data by both 'Title' and 'Publisher' to identify duplicates correctly\n",
    "clean_df = clean_df.groupby(['Title', 'Publisher'], as_index=False).apply(choose_best_duplicate).reset_index(drop=True)\n",
    "\n",
    "# Print removal statistics to show how many rows were deleted and how many remain\n",
    "print_removal_stats(df_before, clean_df, \"Deduplication removal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a0f483-0284-425b-a659-f8d9f126140a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Identify and remove English titles as the dataset is likely focused on Arabic books\n",
    "print(\"\\nBefore English title removal:\")\n",
    "print(\"Total rows:\", len(clean_df))  # Display the total number of rows before removal\n",
    "\n",
    "# Apply the 'is_english' function to detect titles written in English\n",
    "english_titles = clean_df[clean_df['Title'].apply(is_english)]\n",
    "print(\"Found English titles:\", len(english_titles))  # Show the count of English titles found\n",
    "\n",
    "# If English titles are present, display a sample of them\n",
    "if len(english_titles) > 0:\n",
    "    print(\"Sample of English titles to be removed:\")\n",
    "    print(english_titles['Title'].head())  # Display the first few English titles\n",
    "\n",
    "# Remove English titles from the dataset\n",
    "# The tilde (~) operator negates the condition to keep only non-English titles\n",
    "df_before = clean_df.copy()  # Copy the dataset before removal for comparison\n",
    "clean_df = clean_df[~clean_df['Title'].apply(is_english)]  # Filter out English titles\n",
    "\n",
    "# Print statistics to show how many English titles were removed\n",
    "print_removal_stats(df_before, clean_df, \"Removing English titles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05494d2b-2e0a-402c-8992-14de3170806c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Clean Description\n",
    "print(\"\\nBefore Description cleaning:\")\n",
    "print(\"Null values:\", clean_df['Description'].isna().sum())\n",
    "print(\"'None' string values (including spaces):\", \n",
    "      clean_df['Description'].astype(str).str.strip().isin(['None', 'nan', '']).sum())\n",
    "\n",
    "df_before = clean_df.copy()\n",
    "clean_df = clean_df[\n",
    "    clean_df['Description'].notna() & \n",
    "    ~clean_df['Description'].astype(str).str.strip().isin(['None', 'nan', '']) & \n",
    "    (clean_df['Description'].astype(str).str.strip() != '')\n",
    "]\n",
    "print_removal_stats(df_before, clean_df, \"Removing invalid descriptions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc004468-da5c-41ec-acb8-e29c58cad2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Cleaning Function to remove diacritics, symbols, and punctuation\n",
    "def clean_arabic_text(text):\n",
    "    # Remove Arabic diacritics (Tashkeel)\n",
    "    text = re.sub(r'[\\u0617-\\u061A\\u064B-\\u0652]', '', text)\n",
    "    \n",
    "    # Remove punctuations and special characters\n",
    "    text = re.sub(r\"[!\\\"#\\$%&'\\(\\)\\*\\+,\\-./:;<=>?@\\[\\]\\\\^_`{|}~،؛؟«»]\", '', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to the Description column\n",
    "clean_df['Description'] = clean_df['Description'].astype(str).apply(clean_arabic_text)\n",
    "\n",
    "# Display a sample of the cleaned data\n",
    "print(clean_df[['Description']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2396de9f-c749-4e97-9034-14066ceddaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning to the Title column\n",
    "clean_df['Title'] = clean_df['Title'].astype(str).apply(clean_arabic_text)\n",
    "\n",
    "# Display a sample of the cleaned Title data\n",
    "print(clean_df[['Title']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5a8e54-f883-4bd4-bd1c-4d05aff99ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean Author field\n",
    "df_before = clean_df.copy()\n",
    "clean_df = clean_df[\n",
    "    ~clean_df['Author'].astype(str).str.strip().isin(['لا يوجد', 'None', 'nan', '']) & \n",
    "    clean_df['Author'].notna() & \n",
    "    (clean_df['Author'].astype(str).str.strip() != '')\n",
    "]\n",
    "print_removal_stats(df_before, clean_df, \"Removing invalid authors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9416b2-7554-4be5-aa1f-04613c1075ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Publication Year\n",
    "df_before = clean_df.copy()\n",
    "clean_df = clean_df[\n",
    "    (clean_df['Publication year'] != 0) & \n",
    "    clean_df['Publication year'].notna() & \n",
    "    (clean_df['Publication year'] >= 1800) & \n",
    "    (clean_df['Publication year'] <= 2024)\n",
    "]\n",
    "print_removal_stats(df_before, clean_df, \"Removing invalid years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e9528e-6561-46ba-b9f9-ab3555121f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Category Mapping\n",
    "category_map = {\n",
    "    \"الأدب والخيال\": \"0000000000001\",\n",
    "    \"الكتب الإسلامية\": \"0000000000010\",\n",
    "    \"الاقتصاد والأعمال\": \"0000000000100\",\n",
    "    \"الفلسفة\": \"0000000001000\",\n",
    "    \"الصحافة والإعلام\": \"0000000010000\",\n",
    "    \"الكتب السياسية\": \"0000000100000\",\n",
    "    \"العلوم والطبيعة\": \"0000001000000\",\n",
    "    \"الأسرة والطفل\": \"0000010000000\",\n",
    "    \"السير والمذكرات\": \"0000100000000\",\n",
    "    \"الفنون\": \"0001000000000\",\n",
    "    \"التاريخ والجغرافيا\": \"0010000000000\",\n",
    "    \"الرياضة والتسلية\": \"0100000000000\",\n",
    "    \"الشرع والقانون\": \"1000000000000\"\n",
    "}\n",
    "\n",
    "clean_df['Category'] = clean_df['Category'].map(category_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999628f8-7228-44d6-b8a9-9b3ae909aca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Encode Authors using Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "clean_df['Author'] = label_encoder.fit_transform(clean_df['Author'].astype(str))\n",
    "\n",
    "# Display the number of unique authors\n",
    "unique_authors = clean_df['Author'].unique()\n",
    "print(f\"Number of unique authors: {len(unique_authors)}\")\n",
    "\n",
    "# Display a sample of the encoded data\n",
    "print(clean_df[['Author']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6771cdd9-4346-4d42-8e7d-12b06cbc43fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Publishers using Label Encoding\n",
    "clean_df['Publisher'] = label_encoder.fit_transform(clean_df['Publisher'].astype(str))\n",
    "\n",
    "# Display the number of unique publishers\n",
    "unique_publishers = clean_df['Publisher'].unique()\n",
    "print(f\"Number of unique publishers: {len(unique_publishers)}\")\n",
    "\n",
    "# Display a sample of the encoded data\n",
    "print(clean_df[['Publisher']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4babba4b-42ce-4147-813c-17ba065a6dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Publishers using Label Encoding\n",
    "clean_df['Subcategory'] = label_encoder.fit_transform(clean_df['Subcategory'].astype(str))\n",
    "\n",
    "# Display the number of unique publishers\n",
    "unique_publishers = clean_df['Subcategory'].unique()\n",
    "print(f\"Number of unique publishers: {len(unique_publishers)}\")\n",
    "\n",
    "# Display a sample of the encoded data\n",
    "print(clean_df[['Subcategory']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c6ec21-7aa5-47f5-8ea1-65faac9e85b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Final verification\n",
    "print(\"\\nFinal verification:\")\n",
    "print(\"Checking for English titles:\")\n",
    "remaining_english = clean_df[clean_df['Title'].apply(is_english)]\n",
    "if len(remaining_english) > 0:\n",
    "    print(\"WARNING: Still found English titles:\")\n",
    "    print(remaining_english['Title'].head())\n",
    "else:\n",
    "    print(\"No English titles remaining\")\n",
    "\n",
    "print(\"\\nChecking for 'None' values in Description:\")\n",
    "none_desc = clean_df[clean_df['Description'].astype(str).str.strip().isin(['None', 'nan', ''])]\n",
    "if len(none_desc) > 0:\n",
    "    print(\"WARNING: Still found rows with 'None' in Description:\")\n",
    "    print(none_desc[['Title', 'Description']].head())\n",
    "else:\n",
    "    print(\"No 'None' values found in Description\")\n",
    "\n",
    "print(\"Checking for duplicates\")\n",
    "print_duplicate_stats(clean_df, 'Title')\n",
    "\n",
    "# Save the cleaned dataset\n",
    "# Clean any remaining whitespace\n",
    "for col in clean_df.columns:\n",
    "    if clean_df[col].dtype == object:\n",
    "        clean_df[col] = clean_df[col].astype(str).str.strip()\n",
    "\n",
    "# Save with explicit encoding and quoting\n",
    "clean_df.to_csv(\"~/Downloads/outputnotebook.csv\", \n",
    "                index=False, \n",
    "                encoding='utf-8-sig',\n",
    "                quoting=1)\n",
    "\n",
    "\n",
    "# Verify the saved file\n",
    "print(\"\\nVerifying saved file:\")\n",
    "verification_df = pd.read_csv(\"~/Downloads/outputnotebook.csv\")\n",
    "print(\"Final row count:\", len(verification_df))\n",
    "print(\"\\nSample of final data:\")\n",
    "print(verification_df[['Title', 'Author', 'Description']].head(2).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ed7b3d-d123-4d11-a9aa-8275bc4a9699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Relationships visualization\n",
    "\n",
    "# **Book Count by Category**\n",
    "# do we have enough books for each catogory is a catogory overpresented ?\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(data=clean_df, x='Category', palette='viridis', hue='Category', legend=False)\n",
    "plt.title(\"Book Count by Category\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=90) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c95bef0-8ed8-4b73-8716-1feb626dbc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Year vs. Price**\n",
    "# Does newer books are more expensive ?\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.lineplot(data=clean_df, x='Publication year', y='Price', marker='o', hue='Category')\n",
    "plt.title(\"Publication Year vs. Price Trend\")\n",
    "plt.xlabel(\"Publication Year\")\n",
    "plt.ylabel(\"Price ($)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6361aa-3a8d-46e0-8d53-c9073fd5cefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Price vs Category**\n",
    "plt.figure(figsize=(8, 6))\n",
    "df_grouped = clean_df.groupby('Category')['Price'].mean().reset_index()\n",
    "sns.barplot(data=df_grouped, x='Category', y='Price', hue='Category', palette='Set2', legend=False)\n",
    "plt.title(\"Average Price by Category\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Average Price ($)\")\n",
    "plt.xticks(rotation=90)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dd54d5-05fb-4986-b9b1-6341ffb0a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Price vs Pages** \n",
    "bins = [0, 50, 100, 150, 200, float('inf')]  \n",
    "labels = ['0-50', '50-100', '100-150', '150-200', '200+']\n",
    "clean_df['Page Range'] = pd.cut(clean_df['Pages'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "df_grouped = clean_df.groupby('Page Range', observed=False)['Price'].mean().reset_index()\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(data=df_grouped, x='Page Range', y='Price', palette='viridis', hue='Page Range', legend=False)\n",
    "plt.title(\"Average Price by Page Range\")\n",
    "plt.xlabel(\"Page Range\")\n",
    "plt.ylabel(\"Average Price ($)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5714b86f-5539-46ba-bff2-7fae38ad8ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# عرض جميع أسماء الأعمدة في DataFrame\n",
    "columns_list = clean_df.columns.tolist()\n",
    "\n",
    "# عرض قائمة الأعمدة\n",
    "columns_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ee0f32-d20e-40d3-9808-ad0983fad654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9e83a6-2d4f-4f17-8a78-bc8af4e020fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.reset_index(drop=True, inplace=True)\n",
    "display(clean_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366cb534-9d2f-4a52-aae9-05fc15c9b4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset as CSV\n",
    "clean_df.to_csv('Book_Cleaned_Dataset_.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# Or save as Excel\n",
    "clean_df.to_excel('Book_Cleaned_Dataset_.xlsx', index=False)\n",
    "\n",
    "print(\"The cleaned dataset has been saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
